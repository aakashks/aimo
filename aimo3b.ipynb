{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"]},{"cell_type":"markdown","metadata":{},"source":["**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-05-26T18:31:42.486473Z","iopub.status.busy":"2024-05-26T18:31:42.485158Z","iopub.status.idle":"2024-05-26T18:31:42.492539Z","shell.execute_reply":"2024-05-26T18:31:42.49064Z","shell.execute_reply.started":"2024-05-26T18:31:42.486434Z"}},"source":["Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","\n","credits:\n","https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline \\\n","https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama \\\n","https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-10T21:28:28.116289Z"},"trusted":true},"outputs":[],"source":["import time\n","NOTEBOOK_START_TIME = time.time()\n","\n","PRIVATE = False\n","TRAIN_PATH = 'data/ood.csv'\n","# TRAIN_PATH = '/kaggle/input/ai-mathematical-olympiad-prize/train.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TOTAL_TOKENS = 2048\n","\n","BATCH_SIZE = 25\n","LOOP_REPS = 4\n","CODE_PROMPT_COUNT = 12   # somewhere from 1:3 to 2:3 is ok\n","\n","TIME_LIMIT = 31500 if PRIVATE else 6300\n","PER_Q_TIME_LIMIT = 640\n","\n","BEST_COUNT_THRESHOLD = 50   # it was set as np.sqrt(jj)\n","CODE_WITH_TEXT = True\n","\n","TEMP = 0.9\n","TOP_P = 1.0\n","\n","SEED = 30108\n","CFILE = 'code01.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","if not PRIVATE:\n","    class train_env():\n","        def __init__(self, randomize=False):\n","            self.randomlize = randomize\n","            \n","            self.df = pd.read_csv(TRAIN_PATH)\n","            self.df['ground_truth'] = self.df['answer']\n","            self.df['answer'] = -1\n","            \n","            if self.randomlize:\n","                self.df = self.df.reset_index().sample(frac=1).reset_index(drop=True)\n","            \n","            self.predict_called = True\n","            self.counter = 0\n","            self.len = len(self.df)\n","        \n","        \n","        def iter_test(self):\n","             while self.counter<self.len:\n","                if self.predict_called:\n","                    self.predict_called = False\n","                    yield (self.df.loc[[self.counter]][['id','problem']]),(self.df.loc[[self.counter]][['id','answer']])\n","                else:\n","                    print(\"You must call `predict()` successfully before you can continue with `iter_test()`\")\n","                    yield None \n","                \n","        def predict(self, answer):\n","            self.df.loc[self.counter, ('answer')] = answer['answer'].values[0]\n","            self.predict_called = True\n","            self.counter+=1\n","\n","    env = train_env(randomize=True)\n","    iter_test = env.iter_test()\n","else:\n","    # Set up the evaluation API\n","    import aimo\n","\n","    env = aimo.make_env()\n","    iter_test = env.iter_test()"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["DEBUG = False\n","# QUANT = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# %%capture\n","# %set_env CONDA_PREFIX=/opt/conda\n","# !pip install -U --no-index /kaggle/input/uv-package-manager/uv-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !uv pip uninstall torch\n","# !uv pip install --no-index --find-links=/kaggle/input/vllm-wheels -U vllm /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip uninstall torch\n","# !pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n","# !pip install --no-index -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !pip install --no-index -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl --find-links /kaggle/input/vllm-whl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MODEL_NAME = '/scratch/aakash_ks.iitr/models/deepseek/'\n","# MODEL_NAME = '/kaggle/input/deepseek-math'\n","# MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stop_words = [\"```output\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n","# stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","\n","llm = LLM(\n","    model=MODEL_NAME,\n","    dtype=\"half\",\n","    # enforce_eager=True, #to disable CUDA graphs\n","    gpu_memory_utilization=0.9,  #with enforce eager you can use 0.99; else 0.91\n","    swap_space=2,  #CPU RAM per gpu preferable is 2, default is 4\n","    max_model_len=TOTAL_TOKENS,\n","    kv_cache_dtype=\"fp8\",  #auto means same as model dtype. use fp8 to save memory\n","    tensor_parallel_size=2,\n","    max_num_seqs=BATCH_SIZE,\n","    max_seq_len_to_capture=TOTAL_TOKENS//2,\n","    # disable_custom_all_reduce=True, # on T4\n","    seed=SEED,\n",")\n","\n","# if getting CUDA OOM switch to using enforce_eager = True"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# import gc\n","# import torch\n","# torch.backends.cuda.enable_mem_efficient_sdp(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","# import math"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def naive_parse(answer):\n","    out = []\n","    start = False\n","    end = False\n","    for l in reversed(list(answer)):\n","        if l in '0123456789' and not end:\n","            start = True\n","            out.append(l)\n","        else:\n","            if start:\n","                end = True\n","        \n","    out = reversed(out)\n","    return ''.join(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import subprocess\n","\n","def return_last_print(output, n):\n","    lines = output.strip().split('\\n')\n","    if lines:\n","        return lines[n]\n","    else:\n","        return \"\"\n","\n","def process_code(code, return_shell_output=False):\n","    \n","    def repl(match):\n","        if \"real\" not in match.group():\n","            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","        else:\n","            return \"{}{}\".format(match.group()[:-1], ')')\n","    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","\n","    if return_shell_output:\n","        code = code.replace('\\n', '\\n    ')\n","            # Add a try...except block\n","        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n","    \n","    if not return_shell_output:\n","        print(code)\n","    with open(f'{CFILE}', 'w') as fout:\n","        fout.write(code)\n","    \n","    batcmd = 'timeout 5 ' + sys.executable + f' {CFILE}'\n","    try:\n","        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","        return_value = return_last_print(shell_output, -1)\n","        print(shell_output)\n","        if return_shell_output:\n","            if return_value=='FAIL':\n","                CODE_STATUS = False\n","                return_value = return_last_print(shell_output, -2)\n","                if \"not defined\" in return_value:\n","                    return_value+='\\nTry checking the formatting and imports'\n","            else:\n","                CODE_STATUS = True\n","            return return_value, CODE_STATUS  \n","        code_output = round(float(eval(return_value)))\n","    except Exception as e:\n","        print(e,'shell_output')\n","        code_output = -1\n","    \n","    if return_shell_output:\n","        if code_output==-1:\n","            CODE_STATUS = False\n","        else:\n","            CODE_STATUS = True\n","        return code_output, CODE_STATUS  \n","    \n","    \n","    return code_output\n","\n","\n","def process_text_output(output):\n","    result = output    \n","    try:\n","        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n","\n","        print('BOXED', result_output)\n","        if not len(result_output):\n","            result_output = naive_parse(result)\n","        else:\n","            result_output = result_output[-1]\n","\n","        print('BOXED FINAL', result_output)\n","        if not len(result_output):\n","            result_output = -1\n","        \n","        else:\n","            result_output = round(float(eval(result_output)))\n","    \n","    except Exception as e:\n","        print(e)\n","        print('ERROR PARSING TEXT')\n","        result_output = -1\n","    \n","    return result_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["code = \"\"\"Below is a math problem you are to solve (non-negative numeric answer!):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be non-negative integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\"\"\"\n","\n","cot = \"\"\"Below is a math problem you are to solve (non-negative numeric answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","promplt_options = [code,cot]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = llm.get_tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import gc\n","from collections import defaultdict\n","from collections import Counter\n","\n","import numpy as np\n","np.random.seed(SEED)\n","\n","tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numerical answer would always be an integer.'\n","\n","\n","#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n","#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n","\n","temperature = TEMP\n","top_p = TOP_P\n","\n","# temperature_coding = TEMP\n","# top_p_coding = TOP_P\n","batch_size = BATCH_SIZE\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (0, 0)\n","\n","for i, (test, sample_submission) in tqdm(enumerate(iter_test)):\n","    print(f\"Solving problem {i} ...\")\n","    try:\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","        if TIME_SPENT>TIME_LIMIT:\n","            sample_submission['answer'] = 0\n","            env.predict(sample_submission)\n","            break\n","        \n","        Q_START_TIME = time.time()\n","        problem = test['problem'].values[0]\n","        \n","        cot_prompt = f\"User: {cot.format(problem, '{}')}\"\n","        code_prompt = f\"User: {code.format(problem, '{}')}\"\n","        \n","        loop_internal_flag = False\n","        \n","        for ji in range(LOOP_REPS):            \n","            if loop_internal_flag:\n","                break\n","            \n","            time_now = time.time()\n","            \n","            if (time_now - Q_START_TIME) > PER_Q_TIME_LIMIT or (time_now - NOTEBOOK_START_TIME)>TIME_LIMIT:\n","                print(f'BREAKING BECAUSE QUESTION TIME LIMIT EXCEEDED')\n","                sample_submission['answer'] = 0\n","                break\n","            \n","            gc.collect()\n","            \n","            # vllm seed\n","            # seed = 42 + ji*10\n","            \n","            prompts = [cot_prompt] * (batch_size - CODE_PROMPT_COUNT) + [code_prompt] * CODE_PROMPT_COUNT\n","            np.random.shuffle(prompts)\n","            \n","            generation_outputs = llm.generate(prompts, SamplingParams(\n","                stop=stop_words, temperature=temperature, max_tokens=TOTAL_TOKENS, top_p=top_p, \n","                include_stop_str_in_output=True))\n","\n","            decoded_outputs = [prompt + output.outputs[0].text for prompt, output in zip(prompts, generation_outputs)]\n","                \n","            for jk in tqdm(range(batch_size)):\n","                jj = ji*20+jk\n","                    \n","                print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","                \n","                best, best_count = best_stats.get(i,(-1,-1))\n","                if best_count>BEST_COUNT_THRESHOLD:      # jj instead of n_repetitions\n","                    print(\"SKIPPING CAUSE FOUND BEST\")\n","                    loop_internal_flag = True\n","                    break\n","                \n","                time_now = time.time()\n","                    \n","                outputs = total_outputs.get(i,[])\n","                text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","                results = total_results.get(i,[])\n","                answers = total_answers.get(i,[])\n","\n","                try:\n","                    ALREADY_GEN = 0\n","                    code_error = None\n","                    code_error_count = 0\n","                    code_miss_count = 0\n","                    code_output = -1\n","                    was_code = False\n","                    inner_loop_continue_flag = False\n","                    while_break_flag = False\n","                    #initail_message = problem  + tool_instruction \n","                \n","                    prompt = prompts.pop(0)\n","                    current_printed = len(prompt)\n","                    \n","                    print(f\"{jj}_{prompt}\\n\")\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt')\n","                    input_len = len(model_inputs['input_ids'][0])\n","                    input_len2 = len(prompt)\n","\n","                    decoded_output = decoded_outputs.pop(0)\n","                    \n","                    model_inputs = tokenizer(decoded_output, return_tensors='pt')\n","                    ALREADY_GEN = len(model_inputs['input_ids'][0])-input_len\n","                    \n","                    print(f\"{decoded_output[current_printed:]}\\n\")\n","                    current_printed += len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                        \n","                    while_loop_count = 0\n","                    while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","                        \n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","                            code_output, CODE_STATUS = process_code(code_text, return_shell_output=True)\n","                            was_code = True\n","                            print('CODE RESULTS', code_output)\n","                            \n","                            # check if code output is numeric\n","                            try:\n","                                float(eval(code_output))\n","                                code_miss_count = 0\n","                            except:\n","                                code_output = -1\n","                                code_miss_count+=1\n","                                \n","                            # in case when code outputs something like 1/34 and text will add num and denom to give output\n","                            try:\n","                                float(code_output)\n","                                is_float_flag = True\n","                            except:\n","                                is_float_flag = False\n","\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","                                \n","                            if while_loop_count>2:\n","                                print('WHILE LOOP BREAK')\n","                                while_break_flag = True\n","                                break\n","                                \n","                            if code_miss_count>0:\n","                                print('REPEATED CODE MISS')\n","                                inner_loop_continue_flag = True\n","                                break\n","\n","                            if not CODE_STATUS or code_error_count>0 or code_output is None or code_output=='None':\n","                                print('CODE ERROR')\n","                                inner_loop_continue_flag = True\n","                                break\n","                                \n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                        if code_output!=-1:\n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n","                        else:\n","                            prompt = decoded_output\n","                            \n","\n","                        model_inputs = tokenizer(prompt, return_tensors='pt')\n","                        ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","                        \n","                        generation_output = llm.generate([prompt], SamplingParams(\n","                            stop=stop_words, temperature=temperature_inner, max_tokens=TOTAL_TOKENS-ALREADY_GEN, top_p=top_p_inner, \n","                            include_stop_str_in_output=True))\n","                        \n","                        decoded_output = prompt + generation_output[0].outputs[0].text\n","                        \n","                        \n","                        print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                        current_printed+=len(decoded_output[current_printed:])\n","                        \n","                        stop_word_cond = False\n","                        for stop_word in stop_words:\n","                            stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                        while_loop_count+=1\n","                \n","                    try:\n","                        if was_code:\n","                            code_output = round(float(eval(code_output)))\n","\n","                        else:\n","                            code_output = -1\n","                            pass\n","                    except Exception as e:\n","                        print(e,'final_eval')\n","                        code_output = -1\n","\n","                    if was_code and inner_loop_continue_flag:\n","                        continue\n","                    \n","                    raw_output = decoded_output[input_len2:]\n","                    result_output = process_text_output(raw_output)\n","                \n","                    if ALREADY_GEN>=TOTAL_TOKENS-2:\n","                        print('HAD REACHED MAX TOKENS')\n","                        result_output = -1\n","                        \n","                    if while_break_flag:\n","                        print('WHILE BREAK')\n","                        result_output = -1\n","                                                \n","\n","                except Exception as e:\n","                    print(e,\"5\")\n","                    result_output, code_output = -1, -1\n","\n","                if code_output!=-1:\n","                    # ?????\n","                    # this part is doubtful if should be included or not\n","                    # should code output have more weightage than text output ??\n","                    if code_output==result_output:\n","                        print('MATCHED')\n","                        if CODE_WITH_TEXT:\n","                            outputs.append(result_output)\n","                            text_answers+=1\n","                            \n","                        outputs.append(code_output)\n","                        code_answers+=1\n","                        \n","                    else:\n","                        print('NOT MATCHED')\n","                        outputs.append(result_output)\n","                        text_answers+=1\n","                        \n","                        if is_float_flag:\n","                            outputs.append(code_output)\n","                            code_answers+=1\n","                \n","\n","                elif result_output!=-1:\n","                    outputs.append(result_output)\n","                    text_answers+=1\n","\n","                if len(outputs) > 0:\n","                    occurances = Counter(outputs).most_common()\n","                    print(occurances)\n","                    if occurances[0][1] > best_count:\n","                        print(\"GOOD ANSWER UPDATED!\")\n","                        best = occurances[0][0]\n","                        best_count = occurances[0][1]\n","                    if occurances[0][1] > BEST_COUNT_THRESHOLD:\n","                        print(\"ANSWER FOUND!\")\n","                        loop_internal_flag = True\n","                        break\n","\n","                results.append(result_output)\n","                answers.append(code_output)\n","                \n","                best_stats[i] = (best, best_count) \n","                question_type_counts[i] = (text_answers, code_answers)\n","                total_outputs[i] = outputs\n","                \n","                total_results[i] = results\n","                total_answers[i] = answers\n","\n","                print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","                if DEBUG:\n","                    loop_internal_flag = True\n","                    break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0] % 1000\n","        env.predict(sample_submission)\n","    \n","    except Exception as e:\n","        print(e)\n","        sample_submission['answer'] = 0\n","        env.predict(sample_submission)\n","    \n","    print('-' * 80)\n","    print(f'Time spent on the question: {time.time() - Q_START_TIME:.0f} secs')\n","    print('-' * 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if not PRIVATE:\n","    print(env.df)\n","    print(f\"\\ncorrect: {(env.df['ground_truth'] == env.df['answer']).sum()} out of {len(env.df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open(f'{CFILE}', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 5 ' + sys.executable + ' {CFILE}'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f'TOTAL TIME TAKEN BY NB = {time.time() - NOTEBOOK_START_TIME:.0f} secs')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4871830,"sourceId":8218776,"sourceType":"datasetVersion"},{"datasetId":5076335,"sourceId":8505009,"sourceType":"datasetVersion"},{"datasetId":5087488,"sourceId":8650064,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
